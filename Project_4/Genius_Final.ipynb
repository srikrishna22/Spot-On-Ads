{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('/Users/krishna/Desktop/')\n",
    "\n",
    "from genius_credentials import credentials\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "import numpy \n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "# Import all of the scikit learn stuff\n",
    "from __future__ import print_function\n",
    "from sklearn.decomposition import TruncatedSVD, PCA, NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "import pandas as pd\n",
    "\n",
    "# Others\n",
    "from more_itertools import flatten\n",
    "from sklearn.feature_extraction import text \n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "#use this format for working locally\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot, plot_mpl\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_200 = pd.read_csv('./Desktop/regional-us-weekly-latest.csv', header=1)\n",
    "top_200.drop('URL', axis=1, inplace=True)\n",
    "top_200.rename(columns={'Track Name': 'Track_Name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = credentials['client_access_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_search_info(artist, track):\n",
    "    \"\"\"Takes artist and track. Reuturns a song api_path string that you then feed into \n",
    "    the get_referents_info function that then can go get all the annotations \"\"\"\n",
    "    \n",
    "    #set up for search\n",
    "    base_url = 'https://api.genius.com'\n",
    "    headers = {'Authorization': 'Bearer ' + access_token}\n",
    "    search_url = base_url + '/search'\n",
    "    data = {'q': track + ' ' + artist}\n",
    "    \n",
    "    #GET request\n",
    "    response = requests.get(search_url, headers=headers, data=data).json()\n",
    "    \n",
    "    remote_song_info = None\n",
    "    \n",
    "    #check to get the right song api_path by seeing if the artist matches the artist on response\n",
    "    for hit in response['response']['hits']:\n",
    "        if artist.lower() in hit['result']['primary_artist']['name'].lower():\n",
    "            remote_song_info = hit\n",
    "            break\n",
    "    \n",
    "    #if no match return None\n",
    "    #if remote_song_info == None:\n",
    "        #return None\n",
    "    \n",
    "    if remote_song_info:\n",
    "        song_url = remote_song_info['result']['url']\n",
    "        page = requests.get(str(song_url))\n",
    "        html = BeautifulSoup(page.text, 'html.parser')\n",
    "        lyrics = html.find('div', class_='lyrics').get_text().replace('\\n', ' ').strip()\n",
    "    \n",
    "    #split the '/songs/number' string to get number\n",
    "    song_api_path = remote_song_info['result']['api_path'].split('/', maxsplit=2)[2]\n",
    "    \n",
    "    annotations = request_referents_info(song_api_path)\n",
    "   \n",
    "    #return the number\n",
    "    return lyrics, annotations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def request_referents_info(song_api_path):\n",
    "    \"\"\"Takes the song_api_path and gets all the annotations that are attached to \n",
    "    referents\"\"\"\n",
    "    \n",
    "    client_access_token = credentials['client_access_token']\n",
    "\n",
    "    id_ = str(song_api_path)\n",
    "    params = {'song_id': id_, 'text_format':'plain'}\n",
    "    headers = {'Authorization': 'Bearer {}'.format(client_access_token)}\n",
    "    r = requests.get('https://api.genius.com/referents?'+str(song_api_path), headers=headers, params=params)\n",
    "    json = r.json()\n",
    "\n",
    "    annotations = ''\n",
    "    \n",
    "    for i in range(len(json['response']['referents']) - 1):\n",
    "        #iterate over the length of this to get to 'body'\n",
    "        annotations += json['response']['referents'][i]['annotations'][0]['body']['plain']\n",
    "    \n",
    "    annotations.replace('\\n', ' ').replace('  ', ' ')\n",
    "    \n",
    "    return annotations\n",
    "\n",
    "\n",
    "\n",
    "def cleaning(text):\n",
    "\n",
    "    \n",
    "    exclude = set(string.punctuation)\n",
    "\n",
    "    \n",
    "    # remove new line and digits with regular expression\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "    text = re.sub(r'\\d', '', text)\n",
    "    # remove patterns matching url format\n",
    "    url_pattern = r'((http|ftp|https):\\/\\/)?[\\w\\-_]+(\\.[\\w\\-_]+)+([\\w\\-\\.,@?^=%&amp;:/~\\+#]*[\\w\\-\\@?^=%&amp;/~\\+#])?'\n",
    "    text = re.sub(url_pattern, ' ', text)\n",
    "    # remove non-ascii characters\n",
    "    text = ''.join(character for character in text if ord(character) < 128)\n",
    "    # remove punctuations\n",
    "    text = ''.join(character for character in text if character not in exclude)\n",
    "    # standardize white space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # drop capitalization\n",
    "    text = text.lower()\n",
    "    #remove white space\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_lyrics_annotations(df):\n",
    "    \n",
    "    lyrics_list = []\n",
    "    annotations_list = []\n",
    "    artist_list = []\n",
    "    track_list = []\n",
    "    \n",
    "    for i in df.index:\n",
    "        \n",
    "        try: \n",
    "            lyrics, annotations = request_search_info(df.Artist[i], df.Track_Name[i])\n",
    "\n",
    "            \n",
    "        except TypeError:\n",
    "            \n",
    "            print(df.Track_Name[i]) #prints which tracks aren't  \n",
    "            continue\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            \n",
    "            lyrics_list.append(cleaning(lyrics))\n",
    "\n",
    "            annotations_list.append(cleaning(annotations))\n",
    "            \n",
    "            artist_list.append(df.Artist[i])\n",
    "            \n",
    "            track_list.append(df.Track_Name[i])\n",
    "            \n",
    "            \n",
    "    \n",
    "    d = {'artist': artist_list, 'track': track_list, 'lyrics': lyrics_list, 'annotations': annotations_list}\n",
    "            \n",
    "    df=pd.DataFrame(data = d)\n",
    "        \n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def cleaning2(text):\n",
    "    \"\"\"keeps commas and periods\"\"\"\n",
    "    \n",
    "    text = re.sub(r'\\b(?:(?:https?|ftp)://)?\\w[\\w-]*(?:\\.[\\w-]+)+\\S*(?<![.,])', ' ', text.lower())\n",
    "    words = re.findall(r'[a-z.,]+', text)\n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = get_lyrics_annotations(top_200)\n",
    "final_df.to_csv('./Desktop/lyrics.csv')\n",
    "df = pd.read_csv('./Desktop/lyrics.csv', index_col=0)\n",
    "final_df[final_df.annotations == ''].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NMF Modeling***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up data\n",
    "df = pd.read_csv('../Desktop/lyrics.csv', index_col=0)\n",
    "df.annotations.fillna('', inplace=True)\n",
    "df['lyrics_anno'] = df.lyrics + df.annotations\n",
    "\n",
    "artistname_stopwords = [i.split() for i in df.artist]\n",
    "\n",
    "artistname_stopwords= list(flatten(artistname_stopwords))\n",
    "\n",
    "artistname_stopwords = [i.lower() for i in artistname_stopwords]\n",
    "\n",
    "my_additional_stop_words = ['like', 'yeah', 'im', 'dont', 'just', 'got', 'verse', 'chorus', 'know', 'lil' 'uh',\n",
    "                            'ive', 'song', 'line', 'youre', 'hes', 'people', 'track', 'drakes', 'niggas', 'shit', 'thats']+artistname_stopwords\n",
    "\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(my_additional_stop_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=1, max_df=.5, stop_words = stop_words)\n",
    "dtm = vectorizer.fit_transform(df.annotations) \n",
    "\n",
    "lsa = NMF(5)\n",
    "dtm_lsa = lsa.fit_transform(dtm)\n",
    "#dtm_lsa = Normalizer(copy=False).fit_transform(dtm_lsa)\n",
    "\n",
    "df['topic'] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic(dtm_lsa, df):\n",
    "\n",
    "    for i, _  in enumerate(dtm_lsa):\n",
    "\n",
    "        index = int(np.argmax(dtm_lsa[i]))\n",
    "\n",
    "        if index == 0:\n",
    "            # sex, drugs, rap\n",
    "            df.set_value(i, 'topic', 1)\n",
    "        elif index == 1:\n",
    "            # feel good love\n",
    "            df.set_value(i, 'topic', 2)\n",
    "        elif index == 2:\n",
    "            # spanish\n",
    "            df.set_value(i, 'topic', 3)\n",
    "        elif index == 3:\n",
    "            # loved and lost\n",
    "            df.set_value(i, 'topic', 4)\n",
    "        else:\n",
    "            # in a relationship\n",
    "            df.set_value(i, 'topic', 5)\n",
    "\n",
    "            \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_topic(dtm_lsa, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the distribution of topics\n",
    "df.topic.value_counts()\n",
    "\n",
    "# Finding the mean index of each topic\n",
    "print(mean(df[df.topic==1].index))\n",
    "print(mean(df[df.topic==2].index))\n",
    "print(mean(df[df.topic==3].index))\n",
    "print(mean(df[df.topic==4].index))\n",
    "print(mean(df[df.topic==5].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lsa, vectorizer.get_feature_names(), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca.fit(dtm_lsa)\n",
    "\n",
    "pca.explained_variance_ratio_\n",
    "\n",
    "pca.components_\n",
    "\n",
    "pca_components = pca.components_\n",
    "points_to_plot=pca.transform(dtm_lsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, z = points_to_plot[:,0], points_to_plot[:,1], points_to_plot[:,2]\n",
    "\n",
    "trace1 = go.Scatter3d(\n",
    "    x=x,\n",
    "    y=y,\n",
    "    z=z,\n",
    "    text=df.track,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=12,\n",
    "        color=df.topic,                # set color to an array/list of desired values\n",
    "        colorscale='Viridis',   # choose a colorscale\n",
    "        opacity=0.8\n",
    "    ),\n",
    "    hoverinfo='text'\n",
    ")\n",
    "\n",
    "data = [trace1]\n",
    "layout = go.Layout(\n",
    "    margin=dict(\n",
    "        l=0,\n",
    "        r=0,\n",
    "        b=0,\n",
    "        t=0\n",
    "    ), showlegend=True\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig, filename='3d-scatter-colorscale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***LDA Modeling***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from more_itertools import flatten\n",
    "\n",
    "from collections import Counter\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction import text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(analyzer='word',stop_words=stop_words, \n",
    "                                   token_pattern='\\\\b[a-z][a-z]+\\\\b', max_df=.4)\n",
    "\n",
    "\n",
    "count_vectorizer.fit(df.annotations)\n",
    "\n",
    "counts = count_vectorizer.transform(df.annotations).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = matutils.Sparse2Corpus(counts)\n",
    "id2word = dict((v,k) for k, v in count_vectorizer.vocabulary_.items())\n",
    "\n",
    "lda = models.LdaModel(corpus=corpus, num_topics=5, id2word=id2word, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/krishna/Project_4\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
